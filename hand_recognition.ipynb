{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow numpy mediapipe opencv-python tensorflowjs pyautogui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Tugas Akhir\\Program\\jupyter-hands-recognition\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteksi Tangan dengan MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ambil Data Latihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bersiap untuk merekam gesture: ss\n",
      "Tekan 's' untuk mulai merekam...\n",
      "Merekam 100 sampel untuk gesture: ss\n",
      "Selesai merekam ss! Data disimpan.\n",
      "Bersiap untuk merekam gesture: transfer_SS\n",
      "Tekan 's' untuk mulai merekam...\n",
      "Merekam 100 sampel untuk gesture: transfer_SS\n",
      "Selesai merekam transfer_SS! Data disimpan.\n",
      "Semua data gesture telah direkam dan disimpan.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"hand_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Daftar gesture yang akan direkam\n",
    "gestures = [\"ss\", \"transfer_SS\"]\n",
    "num_samples = 100  # Jumlah data per gesture\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "for gesture_name in gestures:\n",
    "    print(f\"Bersiap untuk merekam gesture: {gesture_name}\")\n",
    "    print(\"Tekan 's' untuk mulai merekam...\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        cv2.putText(frame, f\"Tekan 's' untuk rekam {gesture_name}\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Hand Tracking\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "            break\n",
    "    \n",
    "    print(f\"Merekam {num_samples} sampel untuk gesture: {gesture_name}\")\n",
    "    data, labels = [], []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(rgb_frame)\n",
    "        \n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                landmarks = []\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    landmarks.append(lm.x)\n",
    "                    landmarks.append(lm.y)\n",
    "                data.append(landmarks)\n",
    "                labels.append(gesture_name)\n",
    "                \n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        cv2.putText(frame, f\"Recording {gesture_name}: {i+1}/{num_samples}\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Hand Tracking\", frame)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Simpan data\n",
    "    np.save(os.path.join(DATA_DIR, f\"{gesture_name}_data.npy\"), np.array(data))\n",
    "    np.save(os.path.join(DATA_DIR, f\"{gesture_name}_labels.npy\"), np.array(labels))\n",
    "    print(f\"Selesai merekam {gesture_name}! Data disimpan.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Semua data gesture telah direkam dan disimpan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data dan Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [f for f in os.listdir(DATA_DIR) if \"data.npy\" in f]\n",
    "label_files = [f for f in os.listdir(DATA_DIR) if \"labels.npy\" in f]\n",
    "\n",
    "X, y = [], []\n",
    "for file in data_files:\n",
    "    X.append(np.load(os.path.join(DATA_DIR, file)))\n",
    "for file in label_files:\n",
    "    y.append(np.load(os.path.join(DATA_DIR, file)))\n",
    "\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.concatenate(y, axis=0)\n",
    "\n",
    "# Encode label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Reshape untuk LSTM\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Tugas Akhir\\Program\\jupyter-hands-recognition\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Tugas Akhir\\Program\\jupyter-hands-recognition\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From d:\\Tugas Akhir\\Program\\jupyter-hands-recognition\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Tugas Akhir\\Program\\jupyter-hands-recognition\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "25/25 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.4975\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6922 - accuracy: 0.5275\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5000\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.6751 - accuracy: 0.8200\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.8600\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.9525\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9950\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 9.7013e-04 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 8.2182e-04 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 7.5612e-04 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 7.0382e-04 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 6.5899e-04 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 5.7353e-04 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 5.2419e-04 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.8969e-04 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 4.3815e-04 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.9436e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26af4b05a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, activation='relu', input_shape=(1, X.shape[2])),\n",
    "    LSTM(64, return_sequences=False, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(set(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=30, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhand_gesture_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"hand_gesture_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jalankan Model di Jupyter Notebook untuk Prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load kembali model yang telah disimpan\n",
    "model = tf.keras.models.load_model(\"hand_gesture_model.h5\")\n",
    "\n",
    "# Buka kamera untuk mendeteksi tangan dan melakukan prediksi\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)  # Flip agar sesuai dengan tampilan asli\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append(lm.x)\n",
    "                landmarks.append(lm.y)\n",
    "\n",
    "            # Konversi ke numpy array dan reshape agar sesuai input LSTM\n",
    "            landmarks = np.array(landmarks).reshape(1, 1, -1)\n",
    "\n",
    "            # Prediksi menggunakan model\n",
    "            prediction = model.predict(landmarks)\n",
    "            class_index = np.argmax(prediction)\n",
    "            class_label = le.inverse_transform([class_index])[0]\n",
    "\n",
    "            # Tampilkan label hasil prediksi pada layar\n",
    "            cv2.putText(frame, class_label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Gambar landmark tangan\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SS with gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogui\n",
      "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pymsgbox (from pyautogui)\n",
      "  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pytweening>=1.0.4 (from pyautogui)\n",
      "  Downloading pytweening-1.2.0.tar.gz (171 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyscreeze>=0.1.21 (from pyautogui)\n",
      "  Downloading pyscreeze-1.0.1.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pygetwindow>=0.0.5 (from pyautogui)\n",
      "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mouseinfo (from pyautogui)\n",
      "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui)\n",
      "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=9.3.0 in d:\\tugas akhir\\program\\hands-recognition-jupyter\\venv\\lib\\site-packages (from pyscreeze>=0.1.21->pyautogui) (11.1.0)\n",
      "Collecting pyperclip (from mouseinfo->pyautogui)\n",
      "  Downloading pyperclip-1.9.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pyautogui, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, pyperclip, pyrect\n",
      "  Building wheel for pyautogui (pyproject.toml): started\n",
      "  Building wheel for pyautogui (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyautogui: filename=pyautogui-0.9.54-py3-none-any.whl size=37705 sha256=1a7ff46b1b0442b6694e6f57b33fff2dd53052ff703fc70efe98b13be6366abe\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\95\\dc\\b1\\fe122b791e0db8bf439a0e6e1d2628e48f10bf430cae13521b\n",
      "  Building wheel for pygetwindow (setup.py): started\n",
      "  Building wheel for pygetwindow (setup.py): finished with status 'done'\n",
      "  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11079 sha256=0bc1498e3c8469ab834d348d2c74b56d4d8a70749b47014f06f4e3481c03c7d2\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\07\\75\\0b\\7ca0b598eb4c21d43ba4bcc78a0538dfcf803a5997da33bc19\n",
      "  Building wheel for pyscreeze (pyproject.toml): started\n",
      "  Building wheel for pyscreeze (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyscreeze: filename=pyscreeze-1.0.1-py3-none-any.whl size=14480 sha256=1629a99f931ff3ae2ed1abfe8c48caa4d754de3b2819c5ac3371cb866dff40a9\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\cd\\e3\\dd\\267b393d8e8f607e47194942740d080d9bfd835cd4375a3de1\n",
      "  Building wheel for pytweening (setup.py): started\n",
      "  Building wheel for pytweening (setup.py): finished with status 'done'\n",
      "  Created wheel for pytweening: filename=pytweening-1.2.0-py3-none-any.whl size=8029 sha256=343d8a6dffc40e091b0b5cb37342015c1c3b2c873274fe60f478801b569fc845\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\db\\81\\dc\\0d61a3c9614f288e057ab63924e2a49edbeed4ffc916dcda1e\n",
      "  Building wheel for mouseinfo (setup.py): started\n",
      "  Building wheel for mouseinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10902 sha256=6da16786641dd046e6e2602bc57435b0bc7143e17bd3e62b928f7352a61047cb\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\20\\0b\\7f\\939ac9ff785b09951c706150537572c00123412f260a6024f3\n",
      "  Building wheel for pymsgbox (pyproject.toml): started\n",
      "  Building wheel for pymsgbox (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pymsgbox: filename=pymsgbox-1.0.9-py3-none-any.whl size=7467 sha256=6c4ca12318d99d48e1d25acf4885818ba8a8d3428df93e1dc074c5c21c92934b\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\85\\92\\63\\e126ee5f33d8f2ed04f96e43ef5df7270a2f331848752e8662\n",
      "  Building wheel for pyperclip (setup.py): started\n",
      "  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.9.0-py3-none-any.whl size=11019 sha256=978ffd469b66dfa2b4e1cf8ea664031ef9bac89e031f9afa864add17afb29296\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\e8\\e7\\56\\591cb88ba1783b38c40d584026e766aac9c3a048e34128ce8b\n",
      "  Building wheel for pyrect (setup.py): started\n",
      "  Building wheel for pyrect (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11205 sha256=800479157670960205f18ab7974fc1718d43f37e3e97616c1f660341a1f9c46e\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\c4\\e9\\fc\\b7a666dd4f9a3168fb44d643079b41d36ddab52f470707e820\n",
      "Successfully built pyautogui pygetwindow pyscreeze pytweening mouseinfo pymsgbox pyperclip pyrect\n",
      "Installing collected packages: pytweening, pyrect, pyperclip, pymsgbox, pyscreeze, pygetwindow, mouseinfo, pyautogui\n",
      "Successfully installed mouseinfo-0.1.3 pyautogui-0.9.54 pygetwindow-0.0.9 pymsgbox-1.0.9 pyperclip-1.9.0 pyrect-0.2.0 pyscreeze-1.0.1 pytweening-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 252ms/step\n",
      "Gesture COPY detected - taking screenshot and uploading...\n",
      "Upload response: {\n",
      "  \"filename\": \"screenshot.png\",\n",
      "  \"status\": \"Image uploaded\"\n",
      "}\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import ImageGrab\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "# --- Inisialisasi Model dan MediaPipe Hands ---\n",
    "model = tf.keras.models.load_model(\"hand_gesture_model.h5\")\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7,\n",
    ")\n",
    "\n",
    "# --- Inisialisasi Label Encoder ---\n",
    "# Ganti dengan label gesture yang kamu punya\n",
    "labels = ['copy', 'paste']  \n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "# --- Fungsi upload screenshot ke API ---\n",
    "def upload_screenshot(img):\n",
    "    _, buffer = cv2.imencode('.png', img)\n",
    "    files = {'file': ('screenshot.png', buffer.tobytes(), 'image/png')}\n",
    "    try:\n",
    "        res = requests.post(\"http://127.0.0.1:5000/api/image\", files=files)\n",
    "        print(\"Upload response:\", res.text)\n",
    "    except Exception as e:\n",
    "        print(\"Upload error:\", e)\n",
    "\n",
    "# --- Setup kamera dan variabel kontrol ---\n",
    "cap = cv2.VideoCapture(0)\n",
    "last_upload_time = 0\n",
    "upload_cooldown = 5  # detik cooldown agar gak spam upload\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # mirror agar tampilan natural\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append(lm.x)\n",
    "                landmarks.append(lm.y)\n",
    "\n",
    "            landmarks = np.array(landmarks).reshape(1, 1, -1)\n",
    "\n",
    "            try:\n",
    "                prediction = model.predict(landmarks)\n",
    "                class_index = np.argmax(prediction)\n",
    "                class_label = le.inverse_transform([class_index])[0]\n",
    "            except Exception as e:\n",
    "                print(\"Prediction error:\", e)\n",
    "                class_label = \"unknown\"\n",
    "\n",
    "            # Tampilkan hasil prediksi di frame\n",
    "            cv2.putText(frame, f\"Gesture: {class_label}\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Jika gesture 'copy' terdeteksi dan cooldown sudah lewat, screenshot dan upload\n",
    "            if class_label.lower() == \"copy\":\n",
    "                current_time = time.time()\n",
    "                if current_time - last_upload_time > upload_cooldown:\n",
    "                    print(\"Gesture COPY detected - taking screenshot and uploading...\")\n",
    "                    screenshot = ImageGrab.grab()\n",
    "                    screenshot_np = np.array(screenshot)\n",
    "                    screenshot_np = cv2.cvtColor(screenshot_np, cv2.COLOR_RGB2BGR)\n",
    "                    upload_screenshot(screenshot_np)\n",
    "                    last_upload_time = current_time\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No hand detected\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
